{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Read Tables"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "courseDF = spark.read.csv('/FileStore/tables/course_name_credits.csv', sep=',', inferSchema='true', header='true')\n\nstudentDF = spark.read.csv('/FileStore/tables/student_name.tsv', sep='\\t', inferSchema='true', header='true')\n\nrelationDF = spark.read.csv('/FileStore/tables/student_course_grade.csv', sep=',', inferSchema='true', header='true')\n\ncourseDF.show()\nstudentDF.show()\nrelationDF.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Join the three tables"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "# relationDF.join(courseDF, courseDF.course == relationDF.course)\n# is equals to\n# relationDF.join(courseDF, courseDF['course'] == relationDF['course'])\n# and the resulting dataframe will have both the two columns from the two table.\n# So you may want to drop a column with dataframe_name.drop('column_you_want_to_drop')\n#\n# Otherwise, if the name of the column you want to join is the same in both tables,\n# you can use the following\njoinedDF = relationDF.join(courseDF, \"course\").join(studentDF, \"student\")\njoinedDF.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Aggregate Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql.functions import *\n\nstudent_avg_grade = joinedDF.groupBy('name').avg('grade') # or .agg(avg('grade'))\ncourse_max_grade = joinedDF.groupBy('course_name').max('grade') # or .agg(max('grade'))\n\nstudent_avg_grade.show()\ncourse_max_grade.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "# use joinedDF['grade'] or col('grade') or joinedDF.grade\nfinalDF = joinedDF.withColumn('grade_minus_4', joinedDF['grade'] - 4)\nfinalDF = finalDF.withColumn('done', when(finalDF['grade_minus_4'] >= 18, 'PASS').otherwise('FAIL'))\nfinalDF = finalDF.select('course_name', 'name', 'done')\nfinalDF.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Save Output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Saving will produce n files, where n is the number of partitions\n# In order to have only a single file, use dataframe_name.coalesce(1).write...\nstudent_avg_grade.write.csv('/FileStore/tables/student_avg_grade.csv')\ncourse_max_grade.write.json('/FileStore/tables/course_max_grade.json')\nfinalDF.write.parquet('/FileStore/tables/finalDF.parquet')"
            ]
        }
    ],
    "metadata": {
        "name": "StudentsExercise",
        "notebookId": 121643447914339
    },
    "nbformat": 4,
    "nbformat_minor": 0
}